{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36734138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "734f9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Data_Prep\n",
    "importlib.reload(Data_Prep)\n",
    "from Data_Prep import Data_Prep, Hitter_IO\n",
    "import Prep_Map\n",
    "importlib.reload(Prep_Map)\n",
    "import Output_Map\n",
    "importlib.reload(Output_Map)\n",
    "\n",
    "data_prep = Data_Prep(Prep_Map.base_prep_map, Output_Map.war_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4233e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_io_list = data_prep.Generate_IO_Hitters(\"WHERE lastMLBSeason<? AND signingYear<? AND isHitter=?\", (2025,2015,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd316be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import torch\n",
    "io_train : list[Hitter_IO]\n",
    "io_test : list[Hitter_IO]\n",
    "io_train, io_test = train_test_split(hitter_io_list, test_size=0.25, random_state=0)\n",
    "\n",
    "train_lengths = torch.tensor([io.length for io in io_train])\n",
    "test_lengths = torch.tensor([io.length for io in io_test])\n",
    "\n",
    "x_train_padded = torch.nn.utils.rnn.pad_sequence([io.input for io in io_train])\n",
    "x_test_padded = torch.nn.utils.rnn.pad_sequence([io.input for io in io_test])\n",
    "y_prospect_train_padded = torch.nn.utils.rnn.pad_sequence([io.output for io in io_train])\n",
    "y_prospect_test_padded = torch.nn.utils.rnn.pad_sequence([io.output for io in io_test])\n",
    "y_stats_train_padded = torch.nn.utils.rnn.pad_sequence([io.stat_output for io in io_train])\n",
    "y_stats_test_padded = torch.nn.utils.rnn.pad_sequence([io.stat_output for io in io_test])\n",
    "y_position_train_padded = torch.nn.utils.rnn.pad_sequence([io.position_output for io in io_train])\n",
    "y_position_test_padded = torch.nn.utils.rnn.pad_sequence([io.position_output for io in io_test])\n",
    "mask_prospect_train_padded = torch.nn.utils.rnn.pad_sequence([io.prospect_mask for io in io_train])\n",
    "mask_prospect_test_padded = torch.nn.utils.rnn.pad_sequence([io.prospect_mask for io in io_test])\n",
    "mask_level_train_padded = torch.nn.utils.rnn.pad_sequence([io.stat_level_mask for io in io_train])\n",
    "mask_level_test_padded = torch.nn.utils.rnn.pad_sequence([io.stat_level_mask for io in io_test])\n",
    "\n",
    "mask_year_train_padded = torch.nn.utils.rnn.pad_sequence([io.year_level_mask for io in io_train])\n",
    "mask_year_test_padded = torch.nn.utils.rnn.pad_sequence([io.year_level_mask for io in io_test])\n",
    "y_year_stats_train_padded = torch.nn.utils.rnn.pad_sequence([io.year_stat_output for io in io_train])\n",
    "y_year_stats_test_padded = torch.nn.utils.rnn.pad_sequence([io.year_stat_output for io in io_test])\n",
    "y_year_position_train_padded = torch.nn.utils.rnn.pad_sequence([io.year_pos_output for io in io_train])\n",
    "y_year_position_test_padded = torch.nn.utils.rnn.pad_sequence([io.year_pos_output for io in io_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66489ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Hitter_Dataset\n",
    "importlib.reload(Hitter_Dataset)\n",
    "from Hitter_Dataset import Hitter_Dataset\n",
    "\n",
    "train_hitters_dataset = Hitter_Dataset(x_train_padded, train_lengths, y_prospect_train_padded, y_stats_train_padded, y_position_train_padded, mask_prospect_train_padded, mask_level_train_padded, mask_year_train_padded, y_year_stats_train_padded, y_year_position_train_padded)\n",
    "test_hitters_dataset = Hitter_Dataset(x_test_padded, test_lengths, y_prospect_test_padded, y_stats_test_padded, y_position_test_padded, mask_prospect_test_padded, mask_level_test_padded, mask_year_test_padded, y_year_stats_test_padded, y_year_position_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f59be1",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d55be864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b3e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Parameters: 38893\n",
      "Epoch [1/1000], Train Loss: 6.3067, Test Loss: 6.4469\n",
      "Epoch [2/1000], Train Loss: 6.3021, Test Loss: 6.4436\n",
      "Epoch [3/1000], Train Loss: 6.2976, Test Loss: 6.4415\n",
      "Epoch [4/1000], Train Loss: 6.2935, Test Loss: 6.4389\n",
      "Epoch [5/1000], Train Loss: 6.2903, Test Loss: 6.4366\n",
      "Epoch [6/1000], Train Loss: 6.2872, Test Loss: 6.4354\n",
      "Epoch [7/1000], Train Loss: 6.2847, Test Loss: 6.4346\n",
      "Epoch [8/1000], Train Loss: 6.2827, Test Loss: 6.4344\n",
      "Epoch [9/1000], Train Loss: 6.2809, Test Loss: 6.4335\n",
      "Epoch [10/1000], Train Loss: 6.2794, Test Loss: 6.4338\n",
      "Epoch [11/1000], Train Loss: 6.2784, Test Loss: 6.4336\n",
      "Epoch [12/1000], Train Loss: 6.2773, Test Loss: 6.4341\n",
      "Epoch [13/1000], Train Loss: 6.2764, Test Loss: 6.4341\n",
      "Epoch [14/1000], Train Loss: 6.2758, Test Loss: 6.4342\n",
      "Epoch [15/1000], Train Loss: 6.2752, Test Loss: 6.4340\n",
      "Epoch [16/1000], Train Loss: 6.2748, Test Loss: 6.4339\n",
      "Epoch [17/1000], Train Loss: 6.2745, Test Loss: 6.4341\n",
      "Epoch [18/1000], Train Loss: 6.2742, Test Loss: 6.4348\n",
      "Epoch [19/1000], Train Loss: 6.2741, Test Loss: 6.4344\n",
      "Epoch [20/1000], Train Loss: 6.2737, Test Loss: 6.4344\n",
      "Epoch [21/1000], Train Loss: 6.2736, Test Loss: 6.4342\n",
      "Epoch [22/1000], Train Loss: 6.2733, Test Loss: 6.4344\n",
      "Epoch [23/1000], Train Loss: 6.2734, Test Loss: 6.4352\n",
      "Epoch [24/1000], Train Loss: 6.2731, Test Loss: 6.4347\n",
      "Epoch [25/1000], Train Loss: 6.2729, Test Loss: 6.4347\n",
      "Epoch [26/1000], Train Loss: 6.2728, Test Loss: 6.4352\n",
      "Epoch [27/1000], Train Loss: 6.2731, Test Loss: 6.4353\n",
      "Epoch [28/1000], Train Loss: 6.2728, Test Loss: 6.4347\n",
      "Epoch [29/1000], Train Loss: 6.2728, Test Loss: 6.4351\n",
      "Epoch [30/1000], Train Loss: 6.2727, Test Loss: 6.4355\n",
      "Epoch [31/1000], Train Loss: 6.2726, Test Loss: 6.4355\n",
      "Epoch [32/1000], Train Loss: 6.2729, Test Loss: 6.4348\n",
      "Epoch [33/1000], Train Loss: 6.2726, Test Loss: 6.4357\n",
      "Epoch [34/1000], Train Loss: 6.2726, Test Loss: 6.4354\n",
      "Epoch [35/1000], Train Loss: 6.2727, Test Loss: 6.4352\n",
      "Epoch [36/1000], Train Loss: 6.2726, Test Loss: 6.4352\n",
      "Epoch [37/1000], Train Loss: 6.2727, Test Loss: 6.4358\n",
      "Epoch [38/1000], Train Loss: 6.2728, Test Loss: 6.4360\n"
     ]
    }
   ],
   "source": [
    "import Hitter_Model\n",
    "importlib.reload(Hitter_Model)\n",
    "from Hitter_Model import RNN_Model, Classification_Loss, Stats_L1_Loss\n",
    "from torch.optim import lr_scheduler\n",
    "import Model_Train\n",
    "importlib.reload(Model_Train)\n",
    "from Model_Train import trainAndGraph\n",
    "from Constants import device\n",
    "\n",
    "batch_size = 200\n",
    "hitting_mutators = data_prep.Generate_Hitting_Mutators(batch_size, Hitter_IO.GetMaxLength(hitter_io_list))\n",
    "\n",
    "num_layers = 3\n",
    "hidden_size = 35\n",
    "network = RNN_Model(x_train_padded[0].shape[1], num_layers, hidden_size, hitting_mutators, output_map=data_prep.output_map)\n",
    "network = network.to(device)\n",
    "\n",
    "print(\"Num. Parameters:\", count_parameters(network))\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=40, cooldown=20, verbose=False)\n",
    "loss_function = Classification_Loss\n",
    "loss_function_stats = Stats_L1_Loss\n",
    "loss_function_position = Hitter_Model.Position_Classification_Loss\n",
    "\n",
    "num_epochs = 1000\n",
    "training_generator = torch.utils.data.DataLoader(train_hitters_dataset, batch_size=batch_size, shuffle=True)\n",
    "testing_generator = torch.utils.data.DataLoader(test_hitters_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainAndGraph(network, training_generator, testing_generator, len(train_hitters_dataset), len(test_hitters_dataset), loss_function, loss_function_stats, loss_function_position, optimizer, scheduler, num_epochs, logging_interval=1, early_stopping_cutoff=40, should_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270061d",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "test_db = sqlite3.connect('test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94917165",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_state_dict(torch.load(\"no_name.pt\"))\n",
    "network.eval()\n",
    "network = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8916/8916 [02:20<00:00, 63.46it/s]\n"
     ]
    }
   ],
   "source": [
    "cursor = test_db.cursor()\n",
    "cursor.execute(\"DELETE FROM StatPred\")\n",
    "cursor.execute(\"DELETE FROM StatAct\")\n",
    "test_db.commit()\n",
    "cursor = test_db\n",
    "\n",
    "softmax = nn.Softmax(dim=0)\n",
    "\n",
    "for io in tqdm(hitter_io_list):\n",
    "    hitter = io.hitter\n",
    "    input = io.input.unsqueeze(0)  \n",
    "    output = io.output\n",
    "    l = torch.tensor([input.shape[1]])\n",
    "    twar, pwar, level, pa, stats, positions, year_stats, year_positions = network(input.to(device), l.to(device))\n",
    "    \n",
    "    for i in range(io.stat_output.size(0)):\n",
    "        position_probs = softmax(positions.squeeze(0)[i,:9])\n",
    "        year_position_probs = softmax(year_positions.squeeze(0)[i,:9])\n",
    "        test_db.execute(\"INSERT INTO StatPred VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\", (hitter.mlbId, 1, 0, io.dates[i,1].item(), io.dates[i,2].item()) + tuple(stats.squeeze(0)[i,:11].tolist()) + tuple(position_probs.tolist()))\n",
    "        test_db.execute(\"INSERT INTO StatAct VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\", (hitter.mlbId, 1, 0, io.dates[i,1].item(), io.dates[i,2].item()) + tuple(io.stat_output[i,:].tolist()) + tuple(io.position_output[i,:].tolist()))\n",
    "        test_db.execute(\"INSERT INTO StatPred VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\", (hitter.mlbId, 1, 1, io.dates[i,1].item(), io.dates[i,2].item()) + tuple(year_stats.squeeze(0)[i,:11].tolist()) + tuple(year_position_probs.tolist()))\n",
    "        test_db.execute(\"INSERT INTO StatAct VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\", (hitter.mlbId, 1, 1, io.dates[i,1].item(), io.dates[i,2].item()) + tuple(io.year_stat_output[i,:].tolist()) + tuple(io.year_pos_output[i,:].tolist()))\n",
    "test_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc00dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Hitter_Model\n",
    "# importlib.reload(Hitter_Model)\n",
    "# from Hitter_Model import RNN_Model, Classification_Loss, Stats_L1_Loss\n",
    "\n",
    "# network = RNN_Model(x_train_padded[0].shape[1], num_layers, hidden_size, hitting_mutators, output_map=data_prep.output_map)\n",
    "# network.load_state_dict(torch.load(\"no_name.pt\"))\n",
    "# network.eval()\n",
    "# network = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def Check_Hitter(id : int, name : str):\n",
    "    with torch.no_grad():\n",
    "        for io in hitter_io_list:\n",
    "            hitter = io.hitter\n",
    "            if (hitter.mlbId == id):\n",
    "                input = io.input.unsqueeze(0)  \n",
    "                output = io.output\n",
    "                l = torch.tensor([input.shape[1]])\n",
    "                twar, pwar, level, pa, stats, positions = network(input.to(device), l.to(device))\n",
    "                \n",
    "                pos_loss = Hitter_Model.Position_Classification_Loss(positions, \n",
    "                                        io.position_output.unsqueeze(0).to(device), \n",
    "                                        io.stat_level_mask.unsqueeze(0).to(device))\n",
    "                torch.set_printoptions(precision=3, sci_mode=False, linewidth=1000, threshold=500000)\n",
    "                print(name)\n",
    "                print(positions.squeeze(0)[:,:9])\n",
    "                #print(F.softmax(twar.squeeze(1), dim=1).cpu())\n",
    "                #print(F.softmax(level.squeeze(0).squeeze(1), dim=1).cpu())\n",
    "                #print(output[0][0].item())\n",
    "                #print(output[0][2].item())\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da7fa0",
   "metadata": {},
   "source": [
    "Get Hitter Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8ccc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Check_Hitter(596146, \"Max Kepler\")\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mCheck_Hitter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m545361\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMike Trout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Check_Hitter(518769, \"Michael Harrington\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Check_Hitter(542454, \"Danny Santana\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Check_Hitter(605381, \"Levi Michael\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m, in \u001b[0;36mCheck_Hitter\u001b[1;34m(id, name)\u001b[0m\n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39moutput\n\u001b[0;32m     10\u001b[0m l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m---> 11\u001b[0m twar, pwar, level, pa, stats, positions \u001b[38;5;241m=\u001b[39m network(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device), l\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     13\u001b[0m pos_loss \u001b[38;5;241m=\u001b[39m Hitter_Model\u001b[38;5;241m.\u001b[39mPosition_Classification_Loss(positions, \n\u001b[0;32m     14\u001b[0m                         io\u001b[38;5;241m.\u001b[39mposition_output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     15\u001b[0m                         io\u001b[38;5;241m.\u001b[39mstat_level_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_printoptions(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, sci_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500000\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "#Check_Hitter(596146, \"Max Kepler\")\n",
    "Check_Hitter(545361, \"Mike Trout\")\n",
    "#Check_Hitter(518769, \"Michael Harrington\")\n",
    "#Check_Hitter(542454, \"Danny Santana\")\n",
    "#Check_Hitter(605381, \"Levi Michael\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
